{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial PySpark Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:49:03.848974Z",
     "start_time": "2017-06-29T15:49:03.810254Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from haversine import haversine\n",
    "from itertools import izip\n",
    "import pyspark as ps\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import struct, udf, col, monotonically_increasing_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session and Initial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:49:06.491776Z",
     "start_time": "2017-06-29T15:49:06.458093Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = ps.sql.SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"local[4]\")\\\n",
    "            .appName(\"Spark_EDA\")\\\n",
    "            .getOrCreate()\n",
    "            \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:49:06.802808Z",
     "start_time": "2017-06-29T15:49:06.761386Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"lat\", FloatType(), True),\n",
    "    StructField(\"lon\", FloatType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"source_id\", StringType(), True),\n",
    "    StructField(\"account_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"created_on\", StringType(), True),\n",
    "    StructField(\"updated_on\", StringType(), True),\n",
    "    StructField(\"start_ts\", IntegerType(), True),\n",
    "    StructField(\"until_ts\", StringType(), True),\n",
    "    StructField(\"report_type\", StringType(), True),\n",
    "    StructField(\"notes\", StringType(), True),\n",
    "    StructField(\"layer_id\", StringType(), True),\n",
    "    StructField(\"severity\", StringType(), True)\n",
    "])\n",
    "\n",
    "target_columns = [\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"start_ts\",\n",
    "    \"report_type\",\n",
    "    \"notes\",\n",
    "    \"severity\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T10:42:15.175259Z",
     "start_time": "2017-06-29T10:42:15.143585Z"
    }
   },
   "source": [
    "### Shell command to clean quoted newlines\n",
    "\n",
    "awk -v RS='\"' 'NR % 2 == 0 { gsub(/\\n/, \" \") } { printf(\"%s%s\", $0, RT) }' input_file > output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:49:07.642681Z",
     "start_time": "2017-06-29T15:49:07.611558Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_filepath = \"../data/reports_12DEC16-26DEC16.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:49:08.615621Z",
     "start_time": "2017-06-29T15:49:08.558400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def severity_score(severity_rating):\n",
    "    if severity_rating == \"low\":\n",
    "        return 1\n",
    "    elif severity_rating == \"moderate\":\n",
    "        return 2\n",
    "    elif severity_rating == \"medium\":\n",
    "        return 3\n",
    "    elif severity_rating == \"high\":\n",
    "        return 4\n",
    "    elif severity_rating == \"extreme\":\n",
    "        return 5\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def severity_score_quadratic(severity_rating):\n",
    "    if severity_rating == \"low\":\n",
    "        return 1\n",
    "    elif severity_rating == \"moderate\":\n",
    "        return 4\n",
    "    elif severity_rating == \"medium\":\n",
    "        return 9\n",
    "    elif severity_rating == \"high\":\n",
    "        return 16\n",
    "    elif severity_rating == \"extreme\":\n",
    "        return 25\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "def severity_score_log(severity_rating):\n",
    "    if severity_rating == \"low\":\n",
    "        return np.log(1)\n",
    "    elif severity_rating == \"moderate\":\n",
    "        return np.log(2)\n",
    "    elif severity_rating == \"medium\":\n",
    "        return np.log(3)\n",
    "    elif severity_rating == \"high\":\n",
    "        return np.log(4)\n",
    "    elif severity_rating == \"extreme\":\n",
    "        return np.log(5)\n",
    "    else:\n",
    "        return np.log(2)\n",
    "    \n",
    "def severity_score_exp(severity_rating):\n",
    "    if severity_rating == \"low\":\n",
    "        return np.exp(1)\n",
    "    elif severity_rating == \"moderate\":\n",
    "        return np.exp(2)\n",
    "    elif severity_rating == \"medium\":\n",
    "        return np.exp(3)\n",
    "    elif severity_rating == \"high\":\n",
    "        return np.exp(4)\n",
    "    elif severity_rating == \"extreme\":\n",
    "        return np.exp(5)\n",
    "    else:\n",
    "        return np.exp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:49:09.128749Z",
     "start_time": "2017-06-29T15:49:09.060783Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "severity_score_udf = udf(lambda severity: \n",
    "                        severity_score(severity), \n",
    "                        FloatType()\n",
    "                    )\n",
    "\n",
    "severity_quadratic_udf = udf(lambda severity: \n",
    "                        severity_score_quadratic(severity), \n",
    "                        FloatType()\n",
    "                    )\n",
    "\n",
    "severity_logarithmic_udf = udf(lambda severity: \n",
    "                        severity_score_log(severity), \n",
    "                        FloatType()\n",
    "                    )\n",
    "\n",
    "severity_exponential_udf = udf(lambda severity: \n",
    "                        severity_score_exp(severity), \n",
    "                        FloatType()\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:56:56.211916Z",
     "start_time": "2017-06-29T15:56:56.116590Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "reports_df = spark.read.csv(data_filepath,\n",
    "                         sep=\"\\t\",\n",
    "                         schema=schema,\n",
    "                         header=None,\n",
    "                         quote='\"')\n",
    "\n",
    "# Drop Nulls - will revisit\n",
    "reports_df = reports_df.dropna()\n",
    "\n",
    "# Keep only target columns\n",
    "reports_df = reports_df.select([column for column in target_columns])\n",
    "\n",
    "# Add index column\n",
    "reports_df = reports_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Create Lat/Long columns\n",
    "reports_df = reports_df.withColumn(\n",
    "    \"lat_long\",\n",
    "    struct(reports_df.lat, reports_df.lon))\n",
    "\n",
    "# Create Severity Features columns\n",
    "\n",
    "# reports_df = reports_df.withColumn(\n",
    "#     \"severity_score\",\n",
    "#     severity_score_udf(col(\"severity\"))\n",
    "# )\n",
    "\n",
    "# reports_df = reports_df.withColumn(\n",
    "#     \"severity_quadratic\",\n",
    "#     severity_quadratic_udf(col(\"severity\"))\n",
    "# )\n",
    "\n",
    "# reports_df = reports_df.withColumn(\n",
    "#     \"severity_log\",\n",
    "#     severity_logarithmic_udf(col(\"severity\"))\n",
    "# )\n",
    "\n",
    "# reports_df = reports_df.withColumn(\n",
    "#     \"severity_exp\",\n",
    "#     severity_exponential_udf(col(\"severity\"))\n",
    "# )\n",
    "\n",
    "\n",
    "# Convert Timestamps to date time groups\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T15:56:57.070793Z",
     "start_time": "2017-06-29T15:56:56.962872Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lon: float (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- start_ts: integer (nullable = true)\n",
      " |-- report_type: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- severity: string (nullable = true)\n",
      " |-- index: long (nullable = false)\n",
      " |-- lat_long: struct (nullable = false)\n",
      " |    |-- lat: float (nullable = true)\n",
      " |    |-- lon: float (nullable = true)\n",
      "\n",
      "+--------+--------+--------------------+--------------------+----------+-----------+--------------------+--------+-----+-------------------+\n",
      "|     lat|     lon|                  id|               title|  start_ts|report_type|               notes|severity|index|           lat_long|\n",
      "+--------+--------+--------------------+--------------------+----------+-----------+--------------------+--------+-----+-------------------+\n",
      "|37.98381|23.72754|KFg4bXSq5hGyhPkeG...|Security Message ...|1479132770|       OSAC|The U.S. Embassy ...| unrated|    0|[37.98381,23.72754]|\n",
      "+--------+--------+--------------------+--------------------+----------+-----------+--------------------+--------+-----+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports_df.printSchema()\n",
    "reports_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Reports with Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cities Data and Build Lat/Long Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T16:20:03.059292Z",
     "start_time": "2017-06-29T16:20:03.032197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities_schema = StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"latitude\", FloatType(), True),\n",
    "        StructField(\"longitude\", FloatType(), True),\n",
    "        StructField(\"country_code\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T16:20:04.181780Z",
     "start_time": "2017-06-29T16:20:04.031592Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+---------+------------+-------------------+-----+\n",
      "|          name|latitude|longitude|country_code|           lat_long|index|\n",
      "+--------------+--------+---------+------------+-------------------+-----+\n",
      "|         Dubai| 25.0657| 55.17128|          AE| [25.0657,55.17128]|    0|\n",
      "|       Sharjah|25.33737| 55.41206|          AE|[25.33737,55.41206]|    1|\n",
      "|        Al Ain|24.19167| 55.76056|          AE|[24.19167,55.76056]|    2|\n",
      "|     Abu Dhabi|24.46667| 54.36667|          AE|[24.46667,54.36667]|    3|\n",
      "|Mazār-e Sharīf|36.70904| 67.11087|          AF|[36.70904,67.11087]|    4|\n",
      "+--------------+--------+---------+------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_df = spark.read.csv(\"../data/cities300000.csv\", header=True, schema=cities_schema)\n",
    "\n",
    "cities_df = cities_df.withColumn(\n",
    "    \"lat_long\", \n",
    "    struct(cities_df.latitude, cities_df.longitude)\n",
    ")\n",
    "cities_df = cities_df.withColumn(\n",
    "    \"index\",\n",
    "    monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "cities_df.persist()\n",
    "cities_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Haversine Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T16:20:06.848086Z",
     "start_time": "2017-06-29T16:20:06.781246Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25.06570053100586, 55.17127990722656)\n"
     ]
    }
   ],
   "source": [
    "for city in cities_df.select(\"lat_long\").toLocalIterator():\n",
    "    print tuple(city[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T16:21:59.835840Z",
     "start_time": "2017-06-29T16:20:10.231330Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "[Errno 54] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-993218cd371b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     distances = [\n\u001b[1;32m      4\u001b[0m         \u001b[0mhaversine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcities_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lat_long\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoLocalIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     ]\n\u001b[1;32m      7\u001b[0m     \u001b[0mcity_label_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mload_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_with_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36m_read_with_length\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_with_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSpecialLengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_OF_DATA_SECTION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/serializers.pyc\u001b[0m in \u001b[0;36mread_int\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brendandorsey/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: [Errno 54] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "city_label_indices = []\n",
    "for report in reports_df.select(\"lat_long\").toLocalIterator():\n",
    "    distances = [\n",
    "        haversine(tuple(report[0]), tuple(city[0])) \n",
    "        for city in cities_df.select(\"lat_long\").toLocalIterator()\n",
    "    ]\n",
    "    city_label_indices.append(np.argmin(distances))\n",
    "    \n",
    "print len(city_label_indices)\n",
    "print reports_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply City Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T16:28:24.994613Z",
     "start_time": "2017-06-29T16:28:24.937064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_label_indices = pd.read_csv(\"../data/city_label_indices.csv\", header=None)\n",
    "city_label_indices = city_label_indices[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T16:32:00.391421Z",
     "start_time": "2017-06-29T16:28:46.385009Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "could not open socket",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-185105810333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcity_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcity_label_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcities_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcities_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcity_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \"\"\"\n\u001b[0;32m--> 935\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \"\"\"\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.1/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36m_load_from_socket\u001b[0;34m(port, serializer)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"could not open socket\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;31m# The RDD materialization time is unpredicable, if we set a timeout for socket reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# operation, it will very possibly fail. See SPARK-18281.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: could not open socket"
     ]
    }
   ],
   "source": [
    "city_labels = []\n",
    "for index in city_label_indices:\n",
    "    result = cities_df.where(cities_df.index == index).select(\"name\").first()\n",
    "    city_labels.append(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T16:42:08.525059Z",
     "start_time": "2017-06-29T16:42:08.097900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98997\n",
      "7641\n",
      "98997\n"
     ]
    }
   ],
   "source": [
    "print len(city_label_indices)\n",
    "print len(city_labels)\n",
    "print reports_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "Show several cities, then Berlin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
